from __future__ import print_function

import sys

import json
import timeit
import datetime
from csv import reader

import numpy as np 

from pyspark import SparkContext
from pyspark import SparkConf

from pyspark.ml import Pipeline
from pyspark.ml.classification import MultilayerPerceptronClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.stat import Statistics

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

from pyspark.ml.linalg import Vectors

if __name__ == "__main__":

    timerstart = timeit.default_timer()

    #Check arguments
    if len(sys.argv) != 4:
        print("Usage: train.py <csv dataset train> <model output path> <csv dataset test>", file=sys.stderr)
        sys.exit(-1)

    # Create spark session
    spark = SparkSession\
        .builder\
        .appName("TrainNeuralNetworkIDS-Python")\
        .getOrCreate()

    # Define dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
    schema = StructType([
        StructField("srcip", StringType(), False),              # Feature 1
        StructField("srcport", IntegerType(), False),           # Feature 2
        StructField("dstip", StringType(), False),              # Feature 3
        StructField("dstport", IntegerType(), False),           # Feature 4
        StructField("proto", IntegerType(), False),             # Feature 5
        StructField("total_fpackets", IntegerType(), False),    # Feature 6
        StructField("total_fvolume", IntegerType(), False),     # Feature 7
        StructField("total_bpackets", IntegerType(), False),    # Feature 8
        StructField("total_bvolume", IntegerType(), False),     # Feature 9
        StructField("min_fpktl", IntegerType(), False),         # Feature 10
        StructField("mean_fpktl", IntegerType(), False),        # Feature 11
        StructField("max_fpktl", IntegerType(), False),         # Feature 12
        StructField("std_fpktl", IntegerType(), False),         # Feature 13
        StructField("min_bpktl", IntegerType(), False),         # Feature 14
        StructField("mean_bpktl", IntegerType(), False),        # Feature 15
        StructField("max_bpktl", IntegerType(), False),         # Feature 16
        StructField("std_bpktl", IntegerType(), False),         # Feature 17
        StructField("min_fiat", IntegerType(), False),          # Feature 18
        StructField("mean_fiat", IntegerType(), False),         # Feature 19
        StructField("max_fiat", IntegerType(), False),          # Feature 20
        StructField("std_fiat", IntegerType(), False),          # Feature 21
        StructField("min_biat", IntegerType(), False),          # Feature 22
        StructField("mean_biat", IntegerType(), False),         # Feature 23
        StructField("max_biat", IntegerType(), False),          # Feature 24
        StructField("std_biat", IntegerType(), False),          # Feature 25
        StructField("duration", IntegerType(), False),          # Feature 26
        StructField("min_active", IntegerType(), False),        # Feature 27
        StructField("mean_active", IntegerType(), False),       # Feature 28
        StructField("max_active", IntegerType(), False),        # Feature 29
        StructField("std_active", IntegerType(), False),        # Feature 30
        StructField("min_idle", IntegerType(), False),          # Feature 31
        StructField("mean_idle", IntegerType(), False),         # Feature 32
        StructField("max_idle", IntegerType(), False),          # Feature 33
        StructField("std_idle", IntegerType(), False),          # Feature 34
        StructField("sflow_fpackets", IntegerType(), False),    # Feature 35
        StructField("sflow_fbytes", IntegerType(), False),      # Feature 36
        StructField("sflow_bpackets", IntegerType(), False),    # Feature 37
        StructField("sflow_bbytes", IntegerType(), False),      # Feature 38
        StructField("fpsh_cnt", IntegerType(), False),          # Feature 39
        StructField("bpsh_cnt", IntegerType(), False),          # Feature 40
        StructField("furg_cnt", IntegerType(), False),          # Feature 41
        StructField("burg_cnt", IntegerType(), False),          # Feature 42
        StructField("total_fhlen", IntegerType(), False),       # Feature 43
        StructField("total_bhlen", IntegerType(), False),       # Feature 44
        StructField("dscp", IntegerType(), False),              # Feature 45
        StructField("label", IntegerType(), False)              # Class Label: 0-Normal; 1-Attack
    ])

    # Load training data
#    data = spark.read.format("csv").load(sys.argv[1], schema=schema)
    data1 = spark.read.csv(sys.argv[1], schema=schema)
    data2 = spark.read.csv(sys.argv[3], schema=schema)

    # Create vector assembler to produce a feature vector for each record for use in MLlib
    # First 45 csv fields are features, the 46th field is the label. Remove IPs from features.
    assembler = VectorAssembler(inputCols=[schema.names[1]]+schema.names[3:-1], outputCol="features")

    # Assemble feature vector in new dataframe
    trainingData = assembler.transform(data1)
    testData = assembler.transform(data2)

    # Specify layers for the neural network:
    # Input layer of size 43 (features). two intermediate of size 5 and 4
    # and output of size 2 (classes)
    layers = [43, 23, 2]

    # Create the trainer and set its parameters
    trainer = MultilayerPerceptronClassifier(maxIter = 50, layers=layers)

    # Train model

    #model = trainer.fit(trainingData.select("features","label"))
    model = trainer.fit(trainingData)

    # Store model
    #model.save(sys.argv[2])
    #model.write().overwrite().save(sys.argv[2]) #overwrite in case model exist
    
    # Compute accuracy on the test set
    predictions = model.transform(testData)
    #predictions = model.transform(assembledData)

    # Select (prediction, true label) and compute metrics
    f1 = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="f1").evaluate(predictions)
    weightedPrecision = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedPrecision").evaluate(predictions)
    weightedRecall = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="weightedRecall").evaluate(predictions)
    accuracy = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy").evaluate(predictions)

    timerend = timeit.default_timer()

    # Save test metrics in a file
    #arquivo = "hdfs://chagas-desktop:9000/user/testes/Results_" + str(sys.argv[1]).split('/app')[1].split('/')[1].split('.csv')[0]
    #spark.sparkContext.parallelize(["f1: "+str(f1), "Weighted Precision: "+str(weightedPrecision), "Weighted Recall: "+str(weightedRecall), "Accuracy: "+str(accuracy), "Time (in seconds): "+str(timerend-timerstart)]).saveAsTextFile(arquivo)

    # Stop spark session
    spark.stop()
    print("\n\n+--------------------------------+")
    print("+          Test Metrics          +")
    print("+--------------------------------+")
    print("| f1                 | %f6 |" % (f1))
    print("| Weighted Precision | %f6 |" % (weightedPrecision))
    print("| Weighted Recall    | %f6 |" % (weightedRecall))
    print("| Accuracy           | %f6 |" % (accuracy))
    print("| Time (in seconds)  | %f6 |" % (timerend-timerstart))
    print("+--------------------------------+\n\n")