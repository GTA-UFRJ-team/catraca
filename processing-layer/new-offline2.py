from __future__ import print_function

import sys

import timeit
import datetime
import numpy as np

from pyspark import SparkContext
from pyspark import SparkConf

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

from pyspark.mllib.stat import Statistics
from pyspark.mllib.linalg import Vectors
from pyspark.mllib.linalg.distributed import RowMatrix

def CorrelationFeature(vectors, schema):

    print("Calculating Correlation")

    vectors_rdd = vectors.rdd.map(lambda row: Vectors.dense([x for x in row["features"]]))

    matriz = spark.sparkContext.broadcast(Statistics.corr(vectors_rdd, method="pearson"))

    summary = Statistics.colStats(vectors_rdd)

    variance = summary.variance()

    ######## Heur√≠stica ########

    w = {}
    aij = {}
    for i in range(len(matriz.value)):
        w[i] = 0
        aij[i] = 0
        for j in np.nan_to_num(matriz.value[i]):
            k = abs(j)
            aij[i] = aij[i] + k
        w[i] = variance[i]/aij[i]

    r = sorted([(value,key) for (key,value) in w.items()], reverse = True)

    index = r[0:6]

    a = []

    for i in index:
        a.append((0,int(i[1])))

    red = MatrixReducer(vectors_rdd, a, schema)

    return red

def MatrixReducer(vector, index, schema):

    def takeElement(vector):
        p = []

        for i in index:
            p.append(vector[i[1]])

        return p

    reducedMatrix = vector.map(lambda x: takeElement(x))
    #vectors2 = reducedMatrix.map(lambda x: np.column_stack(x))

    print(index)
    print("")

    schema2 = StructType([])

    assembledData = reducedMatrix.toDF()

    return assembledData

if __name__ == "__main__":

    timerstart = timeit.default_timer()

    #Check arguments
    if len(sys.argv) != 3:
        print("Usage: train.py <csv dataset> <model output path>", file=sys.stderr)
        sys.exit(-1)

    # Create spark session
    spark = SparkSession\
        .builder\
        .appName("TrainDecisionTreeIDS")\
        .getOrCreate()

    # Define dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
    schema = StructType([
        StructField("srcip", StringType(), False),              # Feature 1
        StructField("srcport", IntegerType(), False),           # Feature 2
        StructField("dstip", StringType(), False),              # Feature 3
        StructField("dstport", IntegerType(), False),           # Feature 4
        StructField("proto", IntegerType(), False),             # Feature 5
        StructField("total_fpackets", IntegerType(), False),    # Feature 6
        StructField("total_fvolume", IntegerType(), False),     # Feature 7
        StructField("total_bpackets", IntegerType(), False),    # Feature 8
        StructField("total_bvolume", IntegerType(), False),     # Feature 9
        StructField("min_fpktl", IntegerType(), False),         # Feature 10
        StructField("mean_fpktl", IntegerType(), False),        # Feature 11
        StructField("max_fpktl", IntegerType(), False),         # Feature 12
        StructField("std_fpktl", IntegerType(), False),         # Feature 13
        StructField("min_bpktl", IntegerType(), False),         # Feature 14
        StructField("mean_bpktl", IntegerType(), False),        # Feature 15
        StructField("max_bpktl", IntegerType(), False),         # Feature 16
        StructField("std_bpktl", IntegerType(), False),         # Feature 17
        StructField("min_fiat", IntegerType(), False),          # Feature 18
        StructField("mean_fiat", IntegerType(), False),         # Feature 19
        StructField("max_fiat", IntegerType(), False),          # Feature 20
        StructField("std_fiat", IntegerType(), False),          # Feature 21
        StructField("min_biat", IntegerType(), False),          # Feature 22
        StructField("mean_biat", IntegerType(), False),         # Feature 23
        StructField("max_biat", IntegerType(), False),          # Feature 24
        StructField("std_biat", IntegerType(), False),          # Feature 25
        StructField("duration", IntegerType(), False),          # Feature 26
        StructField("min_active", IntegerType(), False),        # Feature 27
        StructField("mean_active", IntegerType(), False),       # Feature 28
        StructField("max_active", IntegerType(), False),        # Feature 29
        StructField("std_active", IntegerType(), False),        # Feature 30
        StructField("min_idle", IntegerType(), False),          # Feature 31
        StructField("mean_idle", IntegerType(), False),         # Feature 32
        StructField("max_idle", IntegerType(), False),          # Feature 33
        StructField("std_idle", IntegerType(), False),          # Feature 34
        StructField("sflow_fpackets", IntegerType(), False),    # Feature 35
        StructField("sflow_fbytes", IntegerType(), False),      # Feature 36
        StructField("sflow_bpackets", IntegerType(), False),    # Feature 37
        StructField("sflow_bbytes", IntegerType(), False),      # Feature 38
        StructField("fpsh_cnt", IntegerType(), False),          # Feature 39
        StructField("bpsh_cnt", IntegerType(), False),          # Feature 40
        StructField("furg_cnt", IntegerType(), False),          # Feature 41
        StructField("burg_cnt", IntegerType(), False),          # Feature 42
        StructField("total_fhlen", IntegerType(), False),       # Feature 43
        StructField("total_bhlen", IntegerType(), False),       # Feature 44
        StructField("dscp", IntegerType(), False),              # Feature 45
        StructField("label", StringType(), False)               # Class Label
    ])

    # Load CSV data
    data = spark.read.csv(sys.argv[1], schema=schema)

    # Create vector assembler to produce a feature vector for each record for use in MLlib
    # First 45 csv fields are features, the 46th field is the label. Remove IPs from features.
    assembler = VectorAssembler(inputCols=[schema.names[1]]+schema.names[3:-1], outputCol="features")

    # Assemble feature vector in new dataframe
    assembledData = assembler.transform(data)

    #reducedData = CorrelationFeature(assembledData, schema)

    # Create a label and feature indexers to speed up categorical columns for decision tree
    labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(assembledData)
    featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=20).fit(assembledData)

    # Create a DecisionTree model trainer
    dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures")

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = assembledData.randomSplit([0.7, 0.3])
    
    # Chain indexers and model training in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])

    # Train model
    model = pipeline.fit(trainingData)

    # Store model
    #model.save(sys.argv[2])
    #model.write().overwrite().save(sys.argv[2]) #overwrite in case model exist
    
    # Make predictions
    predictions = model.transform(testData)

    # Select (prediction, true label) and compute metrics
    f1 = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="f1").evaluate(predictions)
    weightedPrecision = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="weightedPrecision").evaluate(predictions)
    weightedRecall = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="weightedRecall").evaluate(predictions)
    accuracy = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy").evaluate(predictions)

    timerend = timeit.default_timer()

    # Stop spark session
    spark.stop()
    print("\n\n+--------------------------------+")
    print("+          Test Metrics          +")
    print("+--------------------------------+")
    print("| f1                 | %f |" % (f1))
    print("| Weighted Precision | %f |" % (weightedPrecision))
    print("| Weighted Recall    | %f |" % (weightedRecall))
    print("| Accuracy           | %f |" % (accuracy))
    print("| Time (in seconds)  | %f |" % (timerend-timerstart))
    print("+--------------------------------+\n\n")