import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf

import org.apache.spark.sql._
import org.apache.spark.sql.types._

import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.ml.linalg.Vectors

object Catraca {

	def main(args: Array[String]) = {

		if(args.length != 2) {
			println("Usage: scala.scl <csv dataset> <model output path>")
			sys.exit(1)
		}

		val csv_dataset = args(0)
		val output_path = args(1)

		// Create spark session
		SparkSession
			.builder
			.appName("TrainDecisionTreeIDS")
			.getOrCreate()

		// Define dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag		
		val schema = StructType(
			StructField("srcip", StringType, false) ::              //Feature 1
			StructField("srcport", IntegerType, false) ::           //Feature 2
			StructField("dstip", StringType, false) ::              //Feature 3
			StructField("dstport", IntegerType, false) ::           //Feature 4
			StructField("proto", IntegerType, false) ::             //Feature 5
			StructField("total_fpackets", IntegerType, false) ::    //Feature 6	
			StructField("total_fvolume", IntegerType, false) ::     //Feature 7
			StructField("total_bpackets", IntegerType, false) ::    //Feature 8
			StructField("total_bvolume", IntegerType, false) ::     //Feature 9
			StructField("min_fpktl", IntegerType, false) ::         //Feature 10
			StructField("mean_fpktl", IntegerType, false) ::        //Feature 11
			StructField("max_fpktl", IntegerType, false) ::         //Feature 12
			StructField("std_fpktl", IntegerType, false) ::         //Feature 13
			StructField("min_bpktl", IntegerType, false) ::         //Feature 14
			StructField("mean_bpktl", IntegerType, false) ::        //Feature 15
			StructField("max_bpktl", IntegerType, false) ::         //Feature 16	
			StructField("std_bpktl", IntegerType, false) ::         //Feature 17
			StructField("min_fiat", IntegerType, false) ::          //Feature 18
			StructField("mean_fiat", IntegerType, false) ::         //Feature 19
			StructField("max_fiat", IntegerType, false) ::          //Feature 20
			StructField("std_fiat", IntegerType, false) ::          //Feature 21
			StructField("min_biat", IntegerType, false) ::          //Feature 22
			StructField("mean_biat", IntegerType, false) ::         //Feature 23
			StructField("max_biat", IntegerType, false) ::          //Feature 24
			StructField("std_biat", IntegerType, false) ::          //Feature 25
			StructField("duration", IntegerType, false) ::          //Feature 26	
			StructField("min_active", IntegerType, false) ::        //Feature 27
			StructField("mean_active", IntegerType, false) ::       //Feature 28
			StructField("max_active", IntegerType, false) ::        //Feature 29
			StructField("std_active", IntegerType, false) ::        //Feature 30
			StructField("min_idle", IntegerType, false) ::          //Feature 31
			StructField("mean_idle", IntegerType, false) ::         //Feature 32
			StructField("max_idle", IntegerType, false) ::          //Feature 33
			StructField("std_idle", IntegerType, false) ::          //Feature 34
			StructField("sflow_fpackets", IntegerType, false) ::    //Feature 35
			StructField("sflow_fbytes", IntegerType, false) ::      //Feature 36	
			StructField("sflow_bpackets", IntegerType, false) ::    //Feature 37
			StructField("sflow_bbytes", IntegerType, false) ::      //Feature 38
			StructField("fpsh_cnt", IntegerType, false) ::          //Feature 39
			StructField("bpsh_cnt", IntegerType, false) ::          //Feature 40
			StructField("furg_cnt", IntegerType, false) ::          //Feature 41
			StructField("burg_cnt", IntegerType, false) ::          //Feature 42
			StructField("total_fhlen", IntegerType, false) ::       //Feature 43
			StructField("total_bhlen", IntegerType, false) ::       //Feature 44
			StructField("dscp", IntegerType, false) ::              //Feature 45
			StructField("label", StringType, false) ::              //Class Label
			Nil)

		// Load CSV data
		data = spark.read.format("csv").option("inferSchema","true").load(csv_dataset)

		// Create vector assembler to produce a feature vector for each record for use in MLlib
    	// First 45 csv fields are features, the 46th field is the label. Remove IPs from features.
		val assembler = new VectorAssembler()
			.setInputCols((List(schema.head):::schema.takeRight(44)).dropRight(1))
			.setOutputCols("features")

		println("Teste conclu√≠do com sucesso")

	}
}
