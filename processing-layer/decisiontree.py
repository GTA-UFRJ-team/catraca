from __future__ import print_function

import sys
import timeit

from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

from pyspark.sql import SparkSession, Row
from pyspark.sql.types import StructType, StructField, StringType, IntegerType

from pyspark.ml.feature import PCA

if __name__ == "__main__":

    #Check arguments
    if len(sys.argv) != 2:
        print("Usage: train.py <csv dataset train>", file=sys.stderr)
        sys.exit(-1)

    # Create spark session
    spark = SparkSession\
        .builder\
        .appName("TrainDecisionTreeIDS-Python")\
        .getOrCreate()

    # Define dataset schema, dataset csv generated by flowtbag https://github.com/DanielArndt/flowtbag
    schema = StructType([
        StructField("srcip", StringType(), False),              # Feature 1
        StructField("srcport", IntegerType(), False),           # Feature 2
        StructField("dstip", StringType(), False),              # Feature 3
        StructField("dstport", IntegerType(), False),           # Feature 4
        StructField("proto", IntegerType(), False),             # Feature 5
        StructField("total_fpackets", IntegerType(), False),    # Feature 6
        StructField("total_fvolume", IntegerType(), False),     # Feature 7
        StructField("total_bpackets", IntegerType(), False),    # Feature 8
        StructField("total_bvolume", IntegerType(), False),     # Feature 9
        StructField("min_fpktl", IntegerType(), False),         # Feature 10
        StructField("mean_fpktl", IntegerType(), False),        # Feature 11
        StructField("max_fpktl", IntegerType(), False),         # Feature 12
        StructField("std_fpktl", IntegerType(), False),         # Feature 13
        StructField("min_bpktl", IntegerType(), False),         # Feature 14
        StructField("mean_bpktl", IntegerType(), False),        # Feature 15
        StructField("max_bpktl", IntegerType(), False),         # Feature 16
        StructField("std_bpktl", IntegerType(), False),         # Feature 17
        StructField("min_fiat", IntegerType(), False),          # Feature 18
        StructField("mean_fiat", IntegerType(), False),         # Feature 19
        StructField("max_fiat", IntegerType(), False),          # Feature 20
        StructField("std_fiat", IntegerType(), False),          # Feature 21
        StructField("min_biat", IntegerType(), False),          # Feature 22
        StructField("mean_biat", IntegerType(), False),         # Feature 23
        StructField("max_biat", IntegerType(), False),          # Feature 24
        StructField("std_biat", IntegerType(), False),          # Feature 25
        StructField("duration", IntegerType(), False),          # Feature 26
        StructField("min_active", IntegerType(), False),        # Feature 27
        StructField("mean_active", IntegerType(), False),       # Feature 28
        StructField("max_active", IntegerType(), False),        # Feature 29
        StructField("std_active", IntegerType(), False),        # Feature 30
        StructField("min_idle", IntegerType(), False),          # Feature 31
        StructField("mean_idle", IntegerType(), False),         # Feature 32
        StructField("max_idle", IntegerType(), False),          # Feature 33
        StructField("std_idle", IntegerType(), False),          # Feature 34
        StructField("sflow_fpackets", IntegerType(), False),    # Feature 35
        StructField("sflow_fbytes", IntegerType(), False),      # Feature 36
        StructField("sflow_bpackets", IntegerType(), False),    # Feature 37
        StructField("sflow_bbytes", IntegerType(), False),      # Feature 38
        StructField("fpsh_cnt", IntegerType(), False),          # Feature 39
        StructField("bpsh_cnt", IntegerType(), False),          # Feature 40
        StructField("furg_cnt", IntegerType(), False),          # Feature 41
        StructField("burg_cnt", IntegerType(), False),          # Feature 42
        StructField("total_fhlen", IntegerType(), False),       # Feature 43
        StructField("total_bhlen", IntegerType(), False),       # Feature 44
        StructField("dscp", IntegerType(), False),              # Feature 45
        StructField("label", IntegerType(), False)              # Class Label: 0-Normal; 1-Attack
    ])

    # Load CSV data
    data = spark.read.csv(sys.argv[1], schema=schema)

    # Create vector assembler to produce a feature vector for each record for use in MLlib
    # First 45 csv fields are features, the 46th field is the label. Remove IPs from features.
    assembler = VectorAssembler(inputCols=[schema.names[1]]+schema.names[3:-1], outputCol="baseFeatures") # Using PCA
    #assembler = VectorAssembler(inputCols=[schema.names[1]]+schema.names[3:-1], outputCol="features") # Not using PCA

    # Assemble feature vectors in new dataframe
    assembledData = assembler.transform(data)

    # Create PCA model (reduce to 6 principal componentes)
    pca = PCA(k=6, inputCol="baseFeatures", outputCol="features")

    timerstart = timeit.default_timer()

    # Reduce assembled data
    model = pca.fit(assembledData)
    reducedData = model.transform(assembledData).select("features","label")

    # Create a label and feature indexers to speed up categorical columns for decision tree
    labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(reducedData)
    featureIndexer = VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=20).fit(reducedData)

    # Create a DecisionTree model trainer
    dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures")

    # Split the data into training and test sets (30% held out for testing)
    (trainingData, testData) = reducedData.randomSplit([0.7, 0.3]) # Using PCA
    #(trainingData, testData) = assembledData.randomSplit([0.7, 0.3]) # Not using PCA
    
    # Chain indexers and model training in a Pipeline
    pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])

    # Train model
    model = pipeline.fit(trainingData)

    timerend = timeit.default_timer()
    
    # Make predictions
    predictions = model.transform(testData)

    # Select (prediction, true label) and compute metrics
    f1 = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="f1").evaluate(predictions)
    precision = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="weightedPrecision").evaluate(predictions)
    recall = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="weightedRecall").evaluate(predictions)
    accuracy = MulticlassClassificationEvaluator(labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy").evaluate(predictions)

    #Time to apply dimensionality redction + time to create model
    totalTime = timerend-timerstart

    # Stop spark session
    spark.stop()
    print("\n\n+-----------------------------------------------+")
    print("+                 Test Metrics                  +")
    print("+-----------------------------------------------+")
    print("| f1                                 | %f |" % (f1))
    print("| Weighted Precision                 | %f |" % (precision))
    print("| Weighted Recall                    | %f |" % (recall))
    print("| Accuracy                           | %f |" % (accuracy))
    print("| Total Time (in seconds)            | %f |" % (totalTime))
    print("+-----------------------------------------------+\n\n")